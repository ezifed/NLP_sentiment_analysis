{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Presentation MSCS3806 12-19-2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtIp0C5dN7x6",
        "colab_type": "text"
      },
      "source": [
        "# **Final Project - MSCS3806: Advanced Topics in AI and Machine Learning**\n",
        "\n",
        "\n",
        "**Professor: Avid Farhoodfar**\n",
        "\n",
        "**Student Name: Siyu Yi**\n",
        "\n",
        "**Student ID: 277727**\n",
        "\n",
        "**Date: December 19, 2019**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H76H9ZyBSPSK",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vS65cbXPzx3",
        "colab_type": "text"
      },
      "source": [
        "Sentiment Analysis is a common task in Natural Language Processing (NLP). One application of sentiment analysis is to predict whether a movie review should be classified as positive or negative using machine learning classification algorithms.\n",
        "\n",
        "This project will present a simplied version of sentiment analysis on movie reviews, compiled by Andrew Maas in 2011.http://ai.stanford.edu/~amaas/data/sentiment/. The dataset is a binary sentiment classification that includes 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
        "\n",
        "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
        "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
        "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
        "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
        "  month     = {June},\n",
        "  year      = {2011},\n",
        "  address   = {Portland, Oregon, USA},\n",
        "  publisher = {Association for Computational Linguistics},\n",
        "  pages     = {142--150},\n",
        "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
        "}\n",
        "\n",
        "\n",
        "Because the dataset divides each category of movie reviews in different folder, I used a complete txt file combined by Aaron Kub in his Guthub page: https://github.com/aaronkub/machine-learning-examples/tree/master/imdb-sentiment-analysis.\n",
        "\n",
        "Other references of this project are:\n",
        "https://machinelearningmastery.com/prepare-movie-review-data-sentiment-analysis/;\n",
        "https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pl9ynJLJjdn",
        "colab_type": "text"
      },
      "source": [
        "# Read in the text and assign the sentiment value to the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M-OCy6B8tWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7d1e566f-7c33-4ad2-f0b4-6faa0c1ae8f1"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "#create a list where the first 12500 values are positive, and the rest of 12500 values are negative\n",
        "sent = ['neg']*25000\n",
        "sent[:12500]= ['pos']*12500\n",
        "\n",
        "#read in the full test text file and assign column name of \"body_text\"\n",
        "data_test = pd.read_csv('full_test.txt', sep='\\r', header=None)\n",
        "data_test.columns = ['body_text']\n",
        "\n",
        "#add another column to match the sentiment with the review\n",
        "data_test['label'] = sent\n",
        "cols = ['label', 'body_text']\n",
        "\n",
        "#we want to rearrange the order of columns so that the label is in front of the review.\n",
        "data_test = data_test[cols]\n",
        "\n",
        "data_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>I saw this film in a sneak preview, and it is delightful. The cinematography is unusually creati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>Bill Paxton has taken the true story of the 1913 US golf open and made a film that is about much...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                                                            body_text\n",
              "0   pos  I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit ...\n",
              "1   pos  Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", w...\n",
              "2   pos  As a recreational golfer with some knowledge of the sport's history, I was pleased with Disney's...\n",
              "3   pos  I saw this film in a sneak preview, and it is delightful. The cinematography is unusually creati...\n",
              "4   pos  Bill Paxton has taken the true story of the 1913 US golf open and made a film that is about much..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O-46UhkJxDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ccca2a0a-5602-4940-d900-c46bf6e1c2a8"
      },
      "source": [
        "#Perform the same command with the training review data\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "#create a list where the first 12500 values are positive, and the rest of 12500 values are negative\n",
        "sent = ['neg']*25000\n",
        "sent[:12500]= ['pos']*12500\n",
        "\n",
        "#read in the full test text file and assign column name of \"body_text\"\n",
        "data_train = pd.read_csv('full_train.txt', sep='\\r', header=None)\n",
        "data_train.columns = ['body_text']\n",
        "\n",
        "#add another column to match the sentiment with the review\n",
        "data_train['label'] = sent\n",
        "cols = ['label', 'body_text']\n",
        "\n",
        "#we want to rearrange the order of columns so that the label is in front of the review.\n",
        "data_train = data_train[cols]\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not giv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                                                            body_text\n",
              "0   pos  Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...\n",
              "1   pos  Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a ...\n",
              "2   pos  Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love s...\n",
              "3   pos  This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not giv...\n",
              "4   pos  This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb5nUjm-KGVv",
        "colab_type": "text"
      },
      "source": [
        "# Clean the review texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Pyeba5EkuV",
        "colab_type": "text"
      },
      "source": [
        "## Remove the punctuation, special character, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44HxLJC7_5y_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "2eb59d26-c873-4c7d-a83c-3a0ce0eb0601"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "#because the texts are obtained from the IMDb website, it contains something\n",
        "#like <br> or <br/>. In this case, those character will be replaced by a space.\n",
        "br = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "#First replace all <br><br/> with space. Then create a new list with all character except for punctuation.\n",
        "def remove_punct(text):\n",
        "    text = [br.sub(\" \", line) for line in text]\n",
        "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    \n",
        "    return text_nopunct\n",
        "\n",
        "#data_train_clean = remove_punct(data_train['body_text'])\n",
        "\n",
        "\n",
        "\n",
        "data_train['body_text_clean'] = data_train['body_text'].apply(lambda x: remove_punct(x.lower()))\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n",
              "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a ...</td>\n",
              "      <td>homelessness or houselessness as george carlin stated has been an issue for years but never a pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love s...</td>\n",
              "      <td>brilliant over acting by lesley ann warren best dramatic hobo lady i have ever seen and love sce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not giv...</td>\n",
              "      <td>this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and ...</td>\n",
              "      <td>this is not the typical mel brooks film it was much less slapstick than most of his movies and a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                      body_text_clean\n",
              "0   pos  ...  bromwell high is a cartoon comedy it ran at the same time as some other programs about school li...\n",
              "1   pos  ...  homelessness or houselessness as george carlin stated has been an issue for years but never a pl...\n",
              "2   pos  ...  brilliant over acting by lesley ann warren best dramatic hobo lady i have ever seen and love sce...\n",
              "3   pos  ...  this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a...\n",
              "4   pos  ...  this is not the typical mel brooks film it was much less slapstick than most of his movies and a...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPW-6wweYscf",
        "colab_type": "text"
      },
      "source": [
        "## Further cleaning the text: tokenize, and stopwords\n",
        "Comparing Stemmerize and Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3vU_bjSSpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d600459-0298-44f9-8ca2-f17ce7ca4e79"
      },
      "source": [
        "#download stopwords packet from nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#create variables for all stopwords in nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "ps = nltk.PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24IMnquKYxxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e3e57c76-c0cc-4f3c-c22e-27d9985d64b4"
      },
      "source": [
        "def token(text):\n",
        "  tokens = re.split('\\W+', text)\n",
        "  text = [word for word in tokens if word not in stopwords]\n",
        "  return text\n",
        "\n",
        "data_train['body_text_tokenize'] = data_train['body_text_clean'].apply(lambda x: token(x))\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n",
              "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school li...</td>\n",
              "      <td>[bromwell, high, cartoon, comedy, ran, time, programs, school, life, teachers, 35, years, teachi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a ...</td>\n",
              "      <td>homelessness or houselessness as george carlin stated has been an issue for years but never a pl...</td>\n",
              "      <td>[homelessness, houselessness, george, carlin, stated, issue, years, never, plan, help, street, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love s...</td>\n",
              "      <td>brilliant over acting by lesley ann warren best dramatic hobo lady i have ever seen and love sce...</td>\n",
              "      <td>[brilliant, acting, lesley, ann, warren, best, dramatic, hobo, lady, ever, seen, love, scenes, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not giv...</td>\n",
              "      <td>this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a...</td>\n",
              "      <td>[easily, underrated, film, inn, brooks, cannon, sure, flawed, give, realistic, view, homelessnes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and ...</td>\n",
              "      <td>this is not the typical mel brooks film it was much less slapstick than most of his movies and a...</td>\n",
              "      <td>[typical, mel, brooks, film, much, less, slapstick, movies, actually, plot, followable, leslie, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                   body_text_tokenize\n",
              "0   pos  ...  [bromwell, high, cartoon, comedy, ran, time, programs, school, life, teachers, 35, years, teachi...\n",
              "1   pos  ...  [homelessness, houselessness, george, carlin, stated, issue, years, never, plan, help, street, c...\n",
              "2   pos  ...  [brilliant, acting, lesley, ann, warren, best, dramatic, hobo, lady, ever, seen, love, scenes, c...\n",
              "3   pos  ...  [easily, underrated, film, inn, brooks, cannon, sure, flawed, give, realistic, view, homelessnes...\n",
              "4   pos  ...  [typical, mel, brooks, film, much, less, slapstick, movies, actually, plot, followable, leslie, ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJZZbTuaaDfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1a995e77-6371-4921-abcf-4c49f328c831"
      },
      "source": [
        "def stemming(text):\n",
        "  text = [ps.stem(word) for word in text]\n",
        "  return text\n",
        "\n",
        "data_train['body_text_stemmed'] = data_train['body_text_tokenize'].apply(lambda x:stemming(x))\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenize</th>\n",
              "      <th>body_text_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n",
              "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school li...</td>\n",
              "      <td>[bromwell, high, cartoon, comedy, ran, time, programs, school, life, teachers, 35, years, teachi...</td>\n",
              "      <td>[bromwel, high, cartoon, comedi, ran, time, program, school, life, teacher, 35, year, teach, pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a ...</td>\n",
              "      <td>homelessness or houselessness as george carlin stated has been an issue for years but never a pl...</td>\n",
              "      <td>[homelessness, houselessness, george, carlin, stated, issue, years, never, plan, help, street, c...</td>\n",
              "      <td>[homeless, houseless, georg, carlin, state, issu, year, never, plan, help, street, consid, human...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love s...</td>\n",
              "      <td>brilliant over acting by lesley ann warren best dramatic hobo lady i have ever seen and love sce...</td>\n",
              "      <td>[brilliant, acting, lesley, ann, warren, best, dramatic, hobo, lady, ever, seen, love, scenes, c...</td>\n",
              "      <td>[brilliant, act, lesley, ann, warren, best, dramat, hobo, ladi, ever, seen, love, scene, cloth, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not giv...</td>\n",
              "      <td>this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a...</td>\n",
              "      <td>[easily, underrated, film, inn, brooks, cannon, sure, flawed, give, realistic, view, homelessnes...</td>\n",
              "      <td>[easili, underr, film, inn, brook, cannon, sure, flaw, give, realist, view, homeless, unlik, say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and ...</td>\n",
              "      <td>this is not the typical mel brooks film it was much less slapstick than most of his movies and a...</td>\n",
              "      <td>[typical, mel, brooks, film, much, less, slapstick, movies, actually, plot, followable, leslie, ...</td>\n",
              "      <td>[typic, mel, brook, film, much, less, slapstick, movi, actual, plot, follow, lesli, ann, warren,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                    body_text_stemmed\n",
              "0   pos  ...  [bromwel, high, cartoon, comedi, ran, time, program, school, life, teacher, 35, year, teach, pro...\n",
              "1   pos  ...  [homeless, houseless, georg, carlin, state, issu, year, never, plan, help, street, consid, human...\n",
              "2   pos  ...  [brilliant, act, lesley, ann, warren, best, dramat, hobo, ladi, ever, seen, love, scene, cloth, ...\n",
              "3   pos  ...  [easili, underr, film, inn, brook, cannon, sure, flaw, give, realist, view, homeless, unlik, say...\n",
              "4   pos  ...  [typic, mel, brook, film, much, less, slapstick, movi, actual, plot, follow, lesli, ann, warren,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9uhFTnScSPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "a6b43a4b-ca84-43c6-d9f5-bdf31e95eec8"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemm(text):\n",
        "    text = [wn.lemmatize(word) for word in text]\n",
        "    return text\n",
        "\n",
        "data_train['body_text_lemm'] = data_train['body_text_tokenize'].apply(lambda x: lemm(x))\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenize</th>\n",
              "      <th>body_text_stemmed</th>\n",
              "      <th>body_text_lemm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pos</td>\n",
              "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school l...</td>\n",
              "      <td>bromwell high is a cartoon comedy it ran at the same time as some other programs about school li...</td>\n",
              "      <td>[bromwell, high, cartoon, comedy, ran, time, programs, school, life, teachers, 35, years, teachi...</td>\n",
              "      <td>[bromwel, high, cartoon, comedi, ran, time, program, school, life, teacher, 35, year, teach, pro...</td>\n",
              "      <td>[bromwell, high, cartoon, comedy, ran, time, program, school, life, teacher, 35, year, teaching,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pos</td>\n",
              "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a ...</td>\n",
              "      <td>homelessness or houselessness as george carlin stated has been an issue for years but never a pl...</td>\n",
              "      <td>[homelessness, houselessness, george, carlin, stated, issue, years, never, plan, help, street, c...</td>\n",
              "      <td>[homeless, houseless, georg, carlin, state, issu, year, never, plan, help, street, consid, human...</td>\n",
              "      <td>[homelessness, houselessness, george, carlin, stated, issue, year, never, plan, help, street, co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pos</td>\n",
              "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love s...</td>\n",
              "      <td>brilliant over acting by lesley ann warren best dramatic hobo lady i have ever seen and love sce...</td>\n",
              "      <td>[brilliant, acting, lesley, ann, warren, best, dramatic, hobo, lady, ever, seen, love, scenes, c...</td>\n",
              "      <td>[brilliant, act, lesley, ann, warren, best, dramat, hobo, ladi, ever, seen, love, scene, cloth, ...</td>\n",
              "      <td>[brilliant, acting, lesley, ann, warren, best, dramatic, hobo, lady, ever, seen, love, scene, cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not giv...</td>\n",
              "      <td>this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a...</td>\n",
              "      <td>[easily, underrated, film, inn, brooks, cannon, sure, flawed, give, realistic, view, homelessnes...</td>\n",
              "      <td>[easili, underr, film, inn, brook, cannon, sure, flaw, give, realist, view, homeless, unlik, say...</td>\n",
              "      <td>[easily, underrated, film, inn, brook, cannon, sure, flawed, give, realistic, view, homelessness...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pos</td>\n",
              "      <td>This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and ...</td>\n",
              "      <td>this is not the typical mel brooks film it was much less slapstick than most of his movies and a...</td>\n",
              "      <td>[typical, mel, brooks, film, much, less, slapstick, movies, actually, plot, followable, leslie, ...</td>\n",
              "      <td>[typic, mel, brook, film, much, less, slapstick, movi, actual, plot, follow, lesli, ann, warren,...</td>\n",
              "      <td>[typical, mel, brook, film, much, le, slapstick, movie, actually, plot, followable, leslie, ann,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                       body_text_lemm\n",
              "0   pos  ...  [bromwell, high, cartoon, comedy, ran, time, program, school, life, teacher, 35, year, teaching,...\n",
              "1   pos  ...  [homelessness, houselessness, george, carlin, stated, issue, year, never, plan, help, street, co...\n",
              "2   pos  ...  [brilliant, acting, lesley, ann, warren, best, dramatic, hobo, lady, ever, seen, love, scene, cl...\n",
              "3   pos  ...  [easily, underrated, film, inn, brook, cannon, sure, flawed, give, realistic, view, homelessness...\n",
              "4   pos  ...  [typical, mel, brook, film, much, le, slapstick, movie, actually, plot, followable, leslie, ann,...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh3hjUKBdJI1",
        "colab_type": "text"
      },
      "source": [
        "Based on the result, I think lemmatize looks better in normalizing the text. The following paragraph will only lemmatize the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGFDig6qssmP",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Random forest classifier, Naive Bayes and Support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SmCAT7EjCrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "92d387eb-0702-41ed-9f0f-7fbe8bf259b8"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "review_train = data_train[['label','body_text']].copy()\n",
        "review_test = data_test[['label','body_text']].copy()\n",
        "\n",
        "br = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "def clean_lem(text):\n",
        "    text = [br.sub(\" \", line) for line in text] # replace <br><br/> with space\n",
        "    text = \"\".join([char.lower() for char in text if char not in string.punctuation]) # remove punctuation\n",
        "    token = re.split('\\W+', text) #split words\n",
        "    text = [wn.lemmatize(word) for word in token if word not in stopwords] # Lemmatized\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsJOJIKUJ33w",
        "colab_type": "text"
      },
      "source": [
        "Because there are total of 50k training and testing dataset, my computer cannot calculate this level of massive dataset. Therefore, I sampled 12500 from the training and testing data, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8GuVDPWf3T1",
        "colab_type": "text"
      },
      "source": [
        "## Testing Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-izrCbK8gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_train_sample = review_train.sample(n=12500)\n",
        "review_test_sample = review_test.sample(n=12500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea8oOt3yM3Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0eGfx4JQboQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ee489ad9-b50b-44ad-d311-004e0e8fc32f"
      },
      "source": [
        "# Applying TfidfVectorizer\n",
        "tfidf_vect_sample = TfidfVectorizer(analyzer = clean_lem, ngram_range=(1,2))\n",
        "tfidf_vect_fit_sample = tfidf_vect_sample.fit(review_train_sample['body_text'])\n",
        "\n",
        "tfidf_train_sample = tfidf_vect_fit_sample.transform(review_train_sample['body_text'])\n",
        "tfidf_test_sample = tfidf_vect_fit_sample.transform(review_test_sample['body_text'])\n",
        "\n",
        "X_train_vect_sample = pd.DataFrame(tfidf_train_sample.toarray())\n",
        "X_test_vect_sample = pd.DataFrame(tfidf_test_sample.toarray())\n",
        "\n",
        "Y_train_sample = review_train_sample['label'].copy()\n",
        "Y_test_sample = review_test_sample['label'].copy()\n",
        "\n",
        "X_train_vect_sample.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>65677</th>\n",
              "      <th>65678</th>\n",
              "      <th>65679</th>\n",
              "      <th>65680</th>\n",
              "      <th>65681</th>\n",
              "      <th>65682</th>\n",
              "      <th>65683</th>\n",
              "      <th>65684</th>\n",
              "      <th>65685</th>\n",
              "      <th>65686</th>\n",
              "      <th>65687</th>\n",
              "      <th>65688</th>\n",
              "      <th>65689</th>\n",
              "      <th>65690</th>\n",
              "      <th>65691</th>\n",
              "      <th>65692</th>\n",
              "      <th>65693</th>\n",
              "      <th>65694</th>\n",
              "      <th>65695</th>\n",
              "      <th>65696</th>\n",
              "      <th>65697</th>\n",
              "      <th>65698</th>\n",
              "      <th>65699</th>\n",
              "      <th>65700</th>\n",
              "      <th>65701</th>\n",
              "      <th>65702</th>\n",
              "      <th>65703</th>\n",
              "      <th>65704</th>\n",
              "      <th>65705</th>\n",
              "      <th>65706</th>\n",
              "      <th>65707</th>\n",
              "      <th>65708</th>\n",
              "      <th>65709</th>\n",
              "      <th>65710</th>\n",
              "      <th>65711</th>\n",
              "      <th>65712</th>\n",
              "      <th>65713</th>\n",
              "      <th>65714</th>\n",
              "      <th>65715</th>\n",
              "      <th>65716</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053637</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.148323</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 65717 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0      1      2      3      4      ...  65712  65713  65714  65715  65716\n",
              "0    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "1    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "2    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "3    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "4    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "\n",
              "[5 rows x 65717 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxAsGeCUTz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKXTytcNcGQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1a9b3f4-7fee-4b3c-9be3-d990e6bb2fff"
      },
      "source": [
        "#Applying random forest classifier\n",
        "rf_sample = RandomForestClassifier(n_estimators = 500, max_depth=None, n_jobs=-1)\n",
        "\n",
        "start = time.time()\n",
        "rf_model_sample = rf_sample.fit(X_train_vect_sample, Y_train_sample)\n",
        "end = time.time()\n",
        "fit_time_sample = (end - start)\n",
        "\n",
        "start = time.time()\n",
        "Y_pred_sample = rf_model_sample.predict(X_test_vect_sample)\n",
        "end = time.time()\n",
        "pred_time_sample = (end - start)\n",
        "\n",
        "precision, recall, fscore, train_support = score(Y_test_sample, Y_pred_sample, pos_label='pos', average='binary')\n",
        "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(fit_time_sample, 3), round(pred_time_sample, 3), round(precision, 3), round(recall, 3), round((Y_pred_sample==Y_test_sample).sum()/len(Y_pred_sample), 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit time: 324.12 / Predict time: 4.553 ---- Precision: 0.861 / Recall: 0.852 / Accuracy: 0.857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpPJGbvPHVVh",
        "colab_type": "text"
      },
      "source": [
        "Random forest has a very long running time to analyze the vectorized data: more than 5 mins. But at least the program didn't crush. It crushes when I tried to analyze the full text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptT05WTieDrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0eee0cb6-06e1-4dcd-8749-0be72688a7e1"
      },
      "source": [
        "#Applying Naive Bayes classifier\n",
        "mnb_sample = MultinomialNB()\n",
        "\n",
        "start = time.time()\n",
        "mnb_model_sample = mnb_sample.fit(X_train_vect_sample, Y_train_sample)\n",
        "end = time.time()\n",
        "fit_time_sample = (end - start)\n",
        "\n",
        "start = time.time()\n",
        "Y_pred_sample = mnb_model_sample.predict(X_test_vect_sample)\n",
        "end = time.time()\n",
        "pred_time_sample = (end - start)\n",
        "\n",
        "precision, recall, fscore, train_support = score(Y_test_sample, Y_pred_sample, pos_label='pos', average='binary')\n",
        "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(fit_time_sample, 3), round(pred_time_sample, 3), round(precision, 3), round(recall, 3), round((Y_pred_sample==Y_test_sample).sum()/len(Y_pred_sample), 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit time: 2.481 / Predict time: 1.346 ---- Precision: 0.862 / Recall: 0.79 / Accuracy: 0.832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmVut16GHnjO",
        "colab_type": "text"
      },
      "source": [
        "Comparing to random forest, naive bayes's running time is neglectable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpBzlGT5tl4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8b7334d-10e8-493d-8ae4-2d94f2fc6e0f"
      },
      "source": [
        "#Test the regularization factor C\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    svm = LinearSVC(C=c)\n",
        "    svm.fit(X_train_vect_sample, Y_train_sample)\n",
        "    print (\"Accuracy for C=%s: %s\" \n",
        "           % (c, accuracy_score(Y_test_sample, svm.predict(X_test_vect_sample))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.01: 0.83856\n",
            "Accuracy for C=0.05: 0.86752\n",
            "Accuracy for C=0.25: 0.87336\n",
            "Accuracy for C=0.5: 0.8692\n",
            "Accuracy for C=1: 0.86464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RgmA7K-MKgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "53eafb19-5985-429b-ae97-ca1dc190375d"
      },
      "source": [
        "#Test the regularization factor C\n",
        "\n",
        "for c in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
        "    svm = LinearSVC(C=c)\n",
        "    svm.fit(X_train_vect_sample, Y_train_sample)\n",
        "    print (\"Accuracy for C=%s: %s\" \n",
        "           % (c, accuracy_score(Y_test_sample, svm.predict(X_test_vect_sample))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.05: 0.86752\n",
            "Accuracy for C=0.1: 0.87184\n",
            "Accuracy for C=0.15: 0.87336\n",
            "Accuracy for C=0.2: 0.87304\n",
            "Accuracy for C=0.25: 0.87336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mnG9dyiNF5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cee5b28b-d88e-47e5-f3d9-9ec839139f29"
      },
      "source": [
        "#Test the regularization factor C\n",
        "\n",
        "for c in [0.25, 0.30, 0.35, 0.40, 0.45, 0.5]:\n",
        "    svm = LinearSVC(C=c)\n",
        "    svm.fit(X_train_vect_sample, Y_train_sample)\n",
        "    print (\"Accuracy for C=%s: %s\" \n",
        "           % (c, accuracy_score(Y_test_sample, svm.predict(X_test_vect_sample))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for C=0.25: 0.87336\n",
            "Accuracy for C=0.3: 0.87376\n",
            "Accuracy for C=0.35: 0.87264\n",
            "Accuracy for C=0.4: 0.87112\n",
            "Accuracy for C=0.45: 0.87016\n",
            "Accuracy for C=0.5: 0.8692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LF8Y15IHzbe",
        "colab_type": "text"
      },
      "source": [
        "When c=0.25, we have the most accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C_MYZsGrmeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef48629e-da84-46ed-f3f8-d51da11241de"
      },
      "source": [
        "#Applying Support Vector Machines\n",
        "svm_sample = LinearSVC(C=0.25)\n",
        "\n",
        "start = time.time()\n",
        "svm_model_sample = svm_sample.fit(X_train_vect_sample, Y_train_sample)\n",
        "end = time.time()\n",
        "fit_time_sample = (end - start)\n",
        "\n",
        "start = time.time()\n",
        "Y_pred_sample = svm_model_sample.predict(X_test_vect_sample)\n",
        "end = time.time()\n",
        "pred_time_sample = (end - start)\n",
        "\n",
        "precision, recall, fscore, train_support = score(Y_test_sample, Y_pred_sample, pos_label='pos', average='binary')\n",
        "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(fit_time_sample, 3), round(pred_time_sample, 3), round(precision, 3), round(recall, 3), round((Y_pred_sample==Y_test_sample).sum()/len(Y_pred_sample), 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit time: 1.901 / Predict time: 0.855 ---- Precision: 0.873 / Recall: 0.874 / Accuracy: 0.873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uztQCaJ_O0Z3",
        "colab_type": "text"
      },
      "source": [
        "As you can see, Random Forest classifier yield the accuracy of 0.857;\n",
        "\n",
        "Naive Bayes has the lowest accuracy: 0.832;\n",
        "\n",
        "Support Vector Machine has the highest accuracy of 0.873, when the C=0.25.\n",
        "\n",
        "As to the run time, SVM is faster than Naive Bayes and Random Forest, and the Random Forest has the slowest run time.\n",
        "\n",
        "Therefore, SVM is the best among the three algorithm to perform sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BApi84A7IUth",
        "colab_type": "text"
      },
      "source": [
        "# Summarize\n",
        "\n",
        "\n",
        "This project is a simplified version of sentiment analysis using machine learning classifier algorithm. I used three algorithm -- Random Forest, Naive Bayes, and Support Vector Machine -- to analyze the vectorized text, but none of them achieved the accuracy of more than 90%.\n",
        "\n",
        "\n",
        "In this project, firstly, I clean up the text by replacing some characters such as < br> and < br/> with space, and removing punctuation using string.punctuation.\n",
        "\n",
        "\n",
        "Further, I use stopwords function in nltk library to eliminate words that are not likely to have significant impact on prediction.\n",
        "\n",
        "The result for the three algorithm suggests that SVM is the best algorithm among the three in that it yields the most accuracy in the least amount of time.\n",
        "\n",
        "\n",
        "More advanced function can be implemented by finding features that will differentiate positive/negative reviews, such as deleting most commonly words but have not significant impact on the result. This can decrease the workload of the program so that more data can be used to train/test.\n",
        "\n",
        "\n",
        "Further, this program can achieve more to classify the review into more categories, such as very negative, negative, neutral, positive, very positive. These are the features that can be exploit more using sentiment analysis."
      ]
    }
  ]
}